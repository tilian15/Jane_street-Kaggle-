{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e2cfbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: lightgbm==4.2.0 in c:\\tools\\python312\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: numpy in c:\\tools\\python312\\lib\\site-packages (from lightgbm==4.2.0) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\tools\\python312\\lib\\site-packages (from lightgbm==4.2.0) (1.14.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: catboost==1.2.7 in c:\\tools\\python312\\lib\\site-packages (1.2.7)\n",
      "Requirement already satisfied: graphviz in c:\\tools\\python312\\lib\\site-packages (from catboost==1.2.7) (0.20.3)\n",
      "Requirement already satisfied: matplotlib in c:\\tools\\python312\\lib\\site-packages (from catboost==1.2.7) (3.9.1.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\tools\\python312\\lib\\site-packages (from catboost==1.2.7) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.24 in c:\\tools\\python312\\lib\\site-packages (from catboost==1.2.7) (2.2.2)\n",
      "Requirement already satisfied: scipy in c:\\tools\\python312\\lib\\site-packages (from catboost==1.2.7) (1.14.1)\n",
      "Requirement already satisfied: plotly in c:\\tools\\python312\\lib\\site-packages (from catboost==1.2.7) (5.24.1)\n",
      "Requirement already satisfied: six in c:\\users\\tilia\\appdata\\roaming\\python\\python312\\site-packages (from catboost==1.2.7) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tilia\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=0.24->catboost==1.2.7) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\tools\\python312\\lib\\site-packages (from pandas>=0.24->catboost==1.2.7) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\tools\\python312\\lib\\site-packages (from pandas>=0.24->catboost==1.2.7) (2024.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\tools\\python312\\lib\\site-packages (from matplotlib->catboost==1.2.7) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\tools\\python312\\lib\\site-packages (from matplotlib->catboost==1.2.7) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\tools\\python312\\lib\\site-packages (from matplotlib->catboost==1.2.7) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\tools\\python312\\lib\\site-packages (from matplotlib->catboost==1.2.7) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\tools\\python312\\lib\\site-packages (from matplotlib->catboost==1.2.7) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\tools\\python312\\lib\\site-packages (from matplotlib->catboost==1.2.7) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\tools\\python312\\lib\\site-packages (from matplotlib->catboost==1.2.7) (3.1.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\tools\\python312\\lib\\site-packages (from plotly->catboost==1.2.7) (9.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: xgboost==2.0.3 in c:\\tools\\python312\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in c:\\tools\\python312\\lib\\site-packages (from xgboost==2.0.3) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\tools\\python312\\lib\\site-packages (from xgboost==2.0.3) (1.14.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: joblib==1.4.2 in c:\\tools\\python312\\lib\\site-packages (1.4.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install lightgbm==4.2.0 -i https://mirrors.aliyun.com/pypi/simple/\n",
    "!pip install catboost==1.2.7 -i https://mirrors.aliyun.com/pypi/simple/\n",
    "!pip install xgboost==2.0.3 -i https://mirrors.aliyun.com/pypi/simple/\n",
    "!pip install joblib==1.4.2 -i https://mirrors.aliyun.com/pypi/simple/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854125f1",
   "metadata": {},
   "source": [
    "# Ce notebook a été réalisé avec les versions 1.14.1 de scipy et 1.26.4 de numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e67bca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.1\n",
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import numpy\n",
    "\n",
    "\n",
    "print(scipy.__version__) \n",
    "print(numpy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52ba399",
   "metadata": {},
   "source": [
    "## Les données que l'on traitent sont énormément volumineuses donc on utilise une fonction de réduction de stockage trouvé sur internet et on importe les librairies nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8e59580",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-14T19:09:01.883904Z",
     "iopub.status.busy": "2024-10-14T19:09:01.882923Z",
     "iopub.status.idle": "2024-10-14T19:09:06.569356Z",
     "shell.execute_reply": "2024-10-14T19:09:06.568280Z"
    },
    "papermill": {
     "duration": 4.694416,
     "end_time": "2024-10-14T19:09:06.571825",
     "exception": false,
     "start_time": "2024-10-14T19:09:01.877409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cbt\n",
    "import numpy as np\n",
    "print(np.__version__)\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def reduce_mem_usage(self, float16_as32=True):\n",
    "    # Calculate the initial memory usage of the dataframe\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:  # Iterate over each column\n",
    "        col_type = df[col].dtype  # Get the column type\n",
    "        if col_type != object and str(col_type) != 'category':  # Exclude object or categorical columns\n",
    "            c_min, c_max = df[col].min(), df[col].max()  # Get the min and max values of the column\n",
    "            if str(col_type)[:3] == 'int':  # For integer types\n",
    "                # Convert to appropriate int type based on range\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:  # For floating point types\n",
    "                # Convert to appropriate float type based on range\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    if float16_as32:  # Use float32 if higher precision is needed\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    \n",
    "    # Calculate and print final memory usage\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a1944e",
   "metadata": {},
   "source": [
    "# On a 9 fichiers qui contiennent chacun des fichiers parquets correspondant le premier au dates 1-169, ensuite 170 - ... Je commence par les fusionner pour n'en avoir plus qu'un seul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3581fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fabbaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du fichier : ./jane-street-real-time-market-data-forecasting/train.parquetbis\\partition_id=0\\part-0.parquet\n",
      "ok\n",
      "Memory usage of dataframe is 654.51 MB\n",
      "Memory usage after optimization is: 435.72 MB\n",
      "Decreased by 33.4%\n",
      "Chargement du fichier : ./jane-street-real-time-market-data-forecasting/train.parquetbis\\partition_id=1\\part-0.parquet\n",
      "ok\n",
      "Memory usage of dataframe is 944.04 MB\n",
      "Memory usage after optimization is: 548.24 MB\n",
      "Decreased by 41.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tilia\\AppData\\Local\\Temp\\ipykernel_30976\\3781661208.py:23: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  full_data = pd.concat(dataframes, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier combiné sauvegardé ici : ./jane-street-real-time-market-data-forecasting/train.parquetbis\\combined_data.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "# Chemin vers le répertoire principal\n",
    "base_dir = \"./jane-street-real-time-market-data-forecasting/train.parquetbis\"\n",
    "\n",
    "# Trouver tous les fichiers \"part-0.parquet\" dans les sous-dossiers \"partition_id=X\"\n",
    "parquet_files = glob(os.path.join(base_dir, \"partition_id=*\", \"part-0.parquet\"))\n",
    "\n",
    "# Liste pour stocker les DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Lire chaque fichier et l'ajouter à la liste\n",
    "for file in parquet_files[:2]:\n",
    "    print(f\"Chargement du fichier : {file}\")\n",
    "    df = pd.read_parquet(file)  # Lire le fichier Parquet\n",
    "    print(\"ok\")\n",
    "    df = reduce_mem_usage(df,False)\n",
    "    dataframes.append(df)       # Ajouter le DataFrame à la liste\n",
    "\n",
    "# Concaténer tous les DataFrames en un seul\n",
    "full_data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Sauvegarder dans un fichier Parquet combiné\n",
    "output_file = os.path.join(base_dir, \"combined_data.parquet\")\n",
    "full_data.to_parquet(output_file)\n",
    "\n",
    "print(f\"Fichier combiné sauvegardé ici : {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce32c30f",
   "metadata": {},
   "source": [
    "# Petit résumé de nos données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5edcb641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try ok\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4748457 entries, 0 to 4748456\n",
      "Data columns (total 92 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   date_id      int16  \n",
      " 1   time_id      int16  \n",
      " 2   symbol_id    int8   \n",
      " 3   weight       float16\n",
      " 4   feature_00   float16\n",
      " 5   feature_01   float16\n",
      " 6   feature_02   float16\n",
      " 7   feature_03   float16\n",
      " 8   feature_04   float16\n",
      " 9   feature_05   float16\n",
      " 10  feature_06   float16\n",
      " 11  feature_07   float16\n",
      " 12  feature_08   float16\n",
      " 13  feature_09   int8   \n",
      " 14  feature_10   int8   \n",
      " 15  feature_11   int16  \n",
      " 16  feature_12   float16\n",
      " 17  feature_13   float16\n",
      " 18  feature_14   float16\n",
      " 19  feature_15   float16\n",
      " 20  feature_16   float16\n",
      " 21  feature_17   float16\n",
      " 22  feature_18   float16\n",
      " 23  feature_19   float16\n",
      " 24  feature_20   float16\n",
      " 25  feature_21   float64\n",
      " 26  feature_22   float16\n",
      " 27  feature_23   float16\n",
      " 28  feature_24   float16\n",
      " 29  feature_25   float16\n",
      " 30  feature_26   float64\n",
      " 31  feature_27   float64\n",
      " 32  feature_28   float16\n",
      " 33  feature_29   float16\n",
      " 34  feature_30   float16\n",
      " 35  feature_31   float64\n",
      " 36  feature_32   float16\n",
      " 37  feature_33   float16\n",
      " 38  feature_34   float16\n",
      " 39  feature_35   float16\n",
      " 40  feature_36   float16\n",
      " 41  feature_37   float16\n",
      " 42  feature_38   float16\n",
      " 43  feature_39   float16\n",
      " 44  feature_40   float16\n",
      " 45  feature_41   float16\n",
      " 46  feature_42   float16\n",
      " 47  feature_43   float16\n",
      " 48  feature_44   float16\n",
      " 49  feature_45   float16\n",
      " 50  feature_46   float16\n",
      " 51  feature_47   float16\n",
      " 52  feature_48   float16\n",
      " 53  feature_49   float16\n",
      " 54  feature_50   float16\n",
      " 55  feature_51   float16\n",
      " 56  feature_52   float16\n",
      " 57  feature_53   float16\n",
      " 58  feature_54   float16\n",
      " 59  feature_55   float16\n",
      " 60  feature_56   float16\n",
      " 61  feature_57   float16\n",
      " 62  feature_58   float16\n",
      " 63  feature_59   float16\n",
      " 64  feature_60   float16\n",
      " 65  feature_61   float16\n",
      " 66  feature_62   float16\n",
      " 67  feature_63   float16\n",
      " 68  feature_64   float16\n",
      " 69  feature_65   float16\n",
      " 70  feature_66   float16\n",
      " 71  feature_67   float16\n",
      " 72  feature_68   float16\n",
      " 73  feature_69   float16\n",
      " 74  feature_70   float16\n",
      " 75  feature_71   float16\n",
      " 76  feature_72   float16\n",
      " 77  feature_73   float16\n",
      " 78  feature_74   float16\n",
      " 79  feature_75   float16\n",
      " 80  feature_76   float16\n",
      " 81  feature_77   float16\n",
      " 82  feature_78   float16\n",
      " 83  responder_0  float16\n",
      " 84  responder_1  float16\n",
      " 85  responder_2  float16\n",
      " 86  responder_3  float16\n",
      " 87  responder_4  float16\n",
      " 88  responder_5  float16\n",
      " 89  responder_6  float16\n",
      " 90  responder_7  float16\n",
      " 91  responder_8  float16\n",
      "dtypes: float16(82), float64(4), int16(3), int8(3)\n",
      "memory usage: 928.3 MB\n",
      "0            0\n",
      "1            0\n",
      "2            0\n",
      "3            0\n",
      "4            0\n",
      "          ... \n",
      "4748452    339\n",
      "4748453    339\n",
      "4748454    339\n",
      "4748455    339\n",
      "4748456    339\n",
      "Name: date_id, Length: 4748457, dtype: int16\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the provided Parquet file to inspect its contents\n",
    "# file_path = './jane-street-real-time-market-data-forecasting/train.parquet/partition_id=2/part-0.parquet'\n",
    "file_path = \"./jane-street-real-time-market-data-forecasting/train.parquetbis/combined_data.parquet\"\n",
    "try:\n",
    "    df = pd.read_parquet(file_path)\n",
    "    print(\"try ok\")\n",
    "    df_info = df.info()  # Capture information about the dataframe structure\n",
    "    df_head = df.head()  # Capture the first few rows to understand its content\n",
    "except Exception as e:\n",
    "    df_info = str(e)\n",
    "    df_head = None\n",
    "\n",
    "df_info, df_head\n",
    "\n",
    "print(df[\"date_id\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acd3364",
   "metadata": {},
   "source": [
    "# On charge les données d'entraînement, les filtre en fonction de certaines dates, puis les sépare en ensembles d'entraînement et de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b932c485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 928.34 MB\n",
      "Memory usage after optimization is: 928.34 MB\n",
      "Decreased by 0.0%\n",
      "         date_id  time_id  symbol_id    weight  feature_00  feature_01  \\\n",
      "4660156      339      848         19  4.386719   -0.357910   -1.135742   \n",
      "4660157      339      848         30  0.913086    0.013420   -1.095703   \n",
      "4660158      339      848         33  1.225586   -0.378906   -1.521484   \n",
      "4660159      339      848         34  1.267578   -0.343262   -1.547852   \n",
      "4660160      339      848         38  2.267578   -0.415039   -1.260742   \n",
      "\n",
      "         feature_02  feature_03  feature_04  feature_05  ...  feature_78  \\\n",
      "4660156   -0.375488    0.005455   -1.241211   -0.139771  ...   -0.118408   \n",
      "4660157   -0.315430   -0.227051   -1.131836   -0.119019  ...   -0.094299   \n",
      "4660158    0.209351   -0.241943   -1.596680   -0.082520  ...   -0.196045   \n",
      "4660159   -0.188599    0.174194   -1.161133   -0.150146  ...   -0.221802   \n",
      "4660160    0.046844   -0.117554   -1.687500   -0.096436  ...    0.178833   \n",
      "\n",
      "         responder_0  responder_1  responder_2  responder_3  responder_4  \\\n",
      "4660156     0.163940     0.134033     1.124023     0.484863     0.255127   \n",
      "4660157     0.637695    -0.011482     1.779297     0.757324     0.274170   \n",
      "4660158    -3.343750    -1.012695    -2.968750    -1.113281    -0.598633   \n",
      "4660159     0.965820     0.551270     0.175049    -0.035828    -0.015152   \n",
      "4660160     0.344727     0.556152     2.404297     0.893066     0.378174   \n",
      "\n",
      "         responder_5  responder_6  responder_7  responder_8  \n",
      "4660156     0.479492     0.008537     0.027130     0.000452  \n",
      "4660157     0.741699     1.014648     0.573730     2.359375  \n",
      "4660158    -1.538086    -0.259521    -0.105957    -0.479980  \n",
      "4660159    -0.425293    -0.115906    -0.012688    -0.144165  \n",
      "4660160     1.023438     0.276855     0.197021     0.424072  \n",
      "\n",
      "[5 rows x 92 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the input data directory\n",
    "# If the local directory exists, use it; otherwise, use the Kaggle input directory\n",
    "input_path = './jane-street-real-time-market-data-forecasting' #if os.path.exists('./jane-street-real-time-market-data-forecasting') #else '/kaggle/input/jane-street-real-time-market-data-forecasting/'\n",
    "#file_path = './jane-street-real-time-market-data-forecasting/train.parquet/partition_id=2/part-0.parquet'\n",
    "file_path = \"./jane-street-real-time-market-data-forecasting/train.parquetbis/combined_data.parquet\"\n",
    "\n",
    "\n",
    "# Flag to determine if the script is in training mode or not\n",
    "TRAINING = True\n",
    "\n",
    "# Define the feature names based on the number of features (79 in this case)\n",
    "feature_names = [f\"feature_{i:02d}\" for i in range(79)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_valid_dates = 100 # Number of validation dates to use\n",
    "#skip_dates = 500 # Number of dates to skip from the beginning of the dataset (initial)\n",
    "skip_dates = 10 #for the working test\n",
    "N_fold = 5 # Number of folds for cross-validation\n",
    "\n",
    "# If in training mode, load the training data\n",
    "if TRAINING:\n",
    "    # Load the training data from a Parquet file\n",
    "    df = pd.read_parquet(file_path) \n",
    "    # df = pd.read_parquet(f'{input_path}/train.parquet2')\n",
    "    \n",
    "    # Reduce memory usage of the DataFrame (function not provided here)\n",
    "    df = reduce_mem_usage(df, False)\n",
    "    \n",
    "    # Filter the DataFrame to include only dates greater than or equal to skip_dates\n",
    "    df = df[df['date_id'] >= skip_dates].reset_index(drop=True)\n",
    "    \n",
    "    # Get unique dates from the DataFrame\n",
    "    dates = df['date_id'].unique()\n",
    "    \n",
    "    # Define validation dates as the last `num_valid_dates` dates\n",
    "    valid_dates = dates[-num_valid_dates:]\n",
    "    \n",
    "    # Define training dates as all dates except the last `num_valid_dates` dates\n",
    "    train_dates = dates[:-num_valid_dates]\n",
    "    \n",
    "    # Display the last few rows of the DataFrame (for debugging purposes)\n",
    "    print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69474fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install grpcio\n",
    "!pip install google\n",
    "!pip install protobuf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7daee48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T19:09:06.601503Z",
     "iopub.status.busy": "2024-10-14T19:09:06.601104Z",
     "iopub.status.idle": "2024-10-14T19:09:31.622404Z",
     "shell.execute_reply": "2024-10-14T19:09:31.621325Z"
    },
    "papermill": {
     "duration": 25.028488,
     "end_time": "2024-10-14T19:09:31.624884",
     "exception": false,
     "start_time": "2024-10-14T19:09:06.596396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a directory to store the trained models\n",
    "os.system('mkdir models')\n",
    "\n",
    "# Define the path to load pre-trained models (if not in training mode)\n",
    "model_path = '/kaggle/input/jsbaselinezyz'\n",
    "\n",
    "\n",
    "TRAINING = True\n",
    "# If in training mode, prepare validation data\n",
    "if TRAINING:\n",
    "    # Extract features, target, and weights for validation dates\n",
    "    X_valid = df[feature_names].loc[df['date_id'].isin(valid_dates)]\n",
    "    y_valid = df['responder_6'].loc[df['date_id'].isin(valid_dates)]\n",
    "    w_valid = df['weight'].loc[df['date_id'].isin(valid_dates)]\n",
    "\n",
    "# Initialize a list to store trained models\n",
    "models = []\n",
    "\n",
    "# Function to train a model or load a pre-trained model\n",
    "def train(model_dict, model_name='lgb'):\n",
    "    if TRAINING:\n",
    "        # Select dates for training based on the fold number\n",
    "        selected_dates = [date for ii, date in enumerate(train_dates) if ii % N_fold != i]\n",
    "        \n",
    "        # Get the model from the dictionary\n",
    "        model = model_dict[model_name]\n",
    "        \n",
    "        # Extract features, target, and weights for the selected training dates\n",
    "        X_train = df[feature_names].loc[df['date_id'].isin(selected_dates)]\n",
    "        y_train = df['responder_6'].loc[df['date_id'].isin(selected_dates)]\n",
    "        w_train = df['weight'].loc[df['date_id'].isin(selected_dates)]\n",
    "\n",
    "        # Train the model based on the type (LightGBM, XGBoost, or CatBoost)\n",
    "        if model_name == 'lgb':\n",
    "\n",
    "            print(\"on est dans le cas lgb\")\n",
    "            # Train LightGBM model with early stopping and evaluation logging\n",
    "            model.fit(X_train, y_train, w_train,  \n",
    "                      eval_metric=[r2_lgb],\n",
    "                      eval_set=[(X_valid, y_valid, w_valid)], \n",
    "                      callbacks=[\n",
    "                          lgb.early_stopping(100), \n",
    "                          lgb.log_evaluation(10)\n",
    "                      ])\n",
    "            \n",
    "        elif model_name == 'cbt':\n",
    "            # Prepare evaluation set for CatBoost\n",
    "            evalset = cbt.Pool(X_valid, y_valid, weight=w_valid)\n",
    "            \n",
    "            # Train CatBoost model with early stopping and verbose logging\n",
    "            model.fit(X_train, y_train, sample_weight=w_train, \n",
    "                      eval_set=[evalset], \n",
    "                      verbose=10, \n",
    "                      early_stopping_rounds=100)\n",
    "            \n",
    "        else:\n",
    "            # Train XGBoost model with early stopping and verbose logging\n",
    "            model.fit(X_train, y_train, sample_weight=w_train, \n",
    "                      eval_set=[(X_valid, y_valid)], \n",
    "                      sample_weight_eval_set=[w_valid], \n",
    "                      verbose=10, \n",
    "                      early_stopping_rounds=100)\n",
    "\n",
    "        # Append the trained model to the list\n",
    "        models.append(model)\n",
    "        \n",
    "        # Save the trained model to a file\n",
    "        joblib.dump(model, f'./models/{model_name}_{i}.model')\n",
    "\n",
    "        print(\"fin du train\")\n",
    "        \n",
    "        # Delete training data to free up memory\n",
    "        del X_train\n",
    "        del y_train\n",
    "        del w_train\n",
    "        \n",
    "        # Collect garbage to free up memory\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        \n",
    "    else:\n",
    "        print(\"test\")\n",
    "        # If not in training mode, load the pre-trained model from the specified path\n",
    "        models.append(joblib.load(f'{model_path}/{model_name}_{i}.model'))\n",
    "        \n",
    "    return \n",
    "\n",
    "# Custom R2 metric for XGBoost\n",
    "def r2_xgb(y_true, y_pred, sample_weight):\n",
    "    r2 = 1 - np.average((y_pred - y_true) ** 2, weights=sample_weight) / (np.average((y_true) ** 2, weights=sample_weight) + 1e-38)\n",
    "    return -r2\n",
    "\n",
    "# Custom R2 metric for LightGBM\n",
    "def r2_lgb(y_true, y_pred, sample_weight):\n",
    "    r2 = 1 - np.average((y_pred - y_true) ** 2, weights=sample_weight) / (np.average((y_true) ** 2, weights=sample_weight) + 1e-38)\n",
    "    return 'r2', r2, True\n",
    "\n",
    "# Custom R2 metric for CatBoost\n",
    "class r2_cbt(object):\n",
    "    def get_final_error(self, error, weight):\n",
    "        return 1 - error / (weight + 1e-38)\n",
    "\n",
    "    def is_max_optimal(self):\n",
    "        return True\n",
    "\n",
    "    def evaluate(self, approxes, target, weight):\n",
    "        assert len(approxes) == 1\n",
    "        assert len(target) == len(approxes[0])\n",
    "\n",
    "        approx = approxes[0]\n",
    "\n",
    "        error_sum = 0.0\n",
    "        weight_sum = 0.0\n",
    "\n",
    "        for i in range(len(approx)):\n",
    "            w = 1.0 if weight is None else weight[i]\n",
    "            weight_sum += w * (target[i] ** 2)\n",
    "            error_sum += w * ((approx[i] - target[i]) ** 2)\n",
    "\n",
    "        return error_sum, weight_sum\n",
    "\n",
    "# Dictionary to store different models with their configurations\n",
    "\n",
    "# premier test : (met bcp trop de temps à s'executer)\n",
    "# model_dict = {\n",
    "#     'lgb': lgb.LGBMRegressor(n_estimators=500, device='gpu', gpu_use_dp=True, objective='l2'),\n",
    "#     'xgb': xgb.XGBRegressor(n_estimators=2000, learning_rate=0.1, max_depth=6, tree_method='hist', device=\"cuda\", objective='reg:squarederror', eval_metric=r2_xgb, disable_default_eval_metric=True),\n",
    "#     'cbt': cbt.CatBoostRegressor(iterations=1000, learning_rate=0.05, task_type='GPU', loss_function='RMSE', eval_metric=r2_cbt()),\n",
    "# }  \n",
    "\n",
    "\n",
    "# model_dict = {\n",
    "#     'lgb': lgb.LGBMRegressor(n_estimators=20, device='gpu', gpu_use_dp=True, objective='l2', max_depth=5, num_leaves=20),\n",
    "#     'xgb': xgb.XGBRegressor(  #si aucun gpu n'est disponible\n",
    "#         n_estimators=50, \n",
    "#         learning_rate=0.2, \n",
    "#         max_depth=4, \n",
    "#         tree_method='hist',  # CPU-friendly option\n",
    "#         objective='reg:squarederror', \n",
    "#         eval_metric=r2_xgb, \n",
    "#         disable_default_eval_metric=True),\n",
    "#     #'xgb': xgb.XGBRegressor(n_estimators=50, learning_rate=0.2, max_depth=4, tree_method='gpu_hist', device=\"cuda\", objective='reg:squarederror', eval_metric=r2_xgb, disable_default_eval_metric=True), #Si un gpu est disponible\n",
    "#     'cbt': cbt.CatBoostRegressor(iterations=50, learning_rate=0.1, depth=6, task_type='GPU', loss_function='RMSE', eval_metric=r2_cbt()),\n",
    "# }\n",
    "\n",
    "\n",
    "# Train models for each fold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9097d8de",
   "metadata": {},
   "source": [
    "## Paramètres des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99f498be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    # LightGBM sans GPU\n",
    "    'lgb': lgb.LGBMRegressor(\n",
    "        n_estimators=20,\n",
    "        device='cpu',  # CPU mode\n",
    "        objective='l2',\n",
    "        max_depth=5,\n",
    "        num_leaves=20\n",
    "    ),\n",
    "    # Version GPU de LightGBM (commentée)\n",
    "    # 'lgb': lgb.LGBMRegressor(\n",
    "    #     n_estimators=20,\n",
    "    #     device='gpu',  # GPU mode\n",
    "    #     gpu_use_dp=True,\n",
    "    #     objective='l2',\n",
    "    #     max_depth=5,\n",
    "    #     num_leaves=20\n",
    "    # ),\n",
    "    \n",
    "    # XGBoost sans GPU\n",
    "    'xgb': xgb.XGBRegressor(  \n",
    "        n_estimators=50, \n",
    "        learning_rate=0.2, \n",
    "        max_depth=4, \n",
    "        tree_method='hist',  # CPU-friendly option\n",
    "        objective='reg:squarederror', \n",
    "        eval_metric=r2_xgb, \n",
    "        disable_default_eval_metric=True\n",
    "    ),\n",
    "    # Version GPU de XGBoost (commentée)\n",
    "    # 'xgb': xgb.XGBRegressor(\n",
    "    #     n_estimators=50,\n",
    "    #     learning_rate=0.2,\n",
    "    #     max_depth=4,\n",
    "    #     tree_method='gpu_hist',  # GPU-friendly option\n",
    "    #     objective='reg:squarederror',\n",
    "    #     eval_metric=r2_xgb,\n",
    "    #     disable_default_eval_metric=True\n",
    "    # ),\n",
    "    \n",
    "    # CatBoost sans GPU\n",
    "    'cbt': cbt.CatBoostRegressor(\n",
    "        iterations=50,\n",
    "        learning_rate=0.1,\n",
    "        depth=6,\n",
    "        task_type='CPU',  # CPU mode\n",
    "        loss_function='RMSE',\n",
    "        eval_metric=r2_cbt()\n",
    "    ),\n",
    "    # Version GPU de CatBoost (commentée)\n",
    "    # 'cbt': cbt.CatBoostRegressor(\n",
    "    #     iterations=50,\n",
    "    #     learning_rate=0.1,\n",
    "    #     depth=6,\n",
    "    #     task_type='GPU',  # GPU mode\n",
    "    #     loss_function='RMSE',\n",
    "    #     eval_metric=r2_cbt()\n",
    "    # ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764cc8a9",
   "metadata": {},
   "source": [
    "## Execution de l'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd38e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(N_fold):\n",
    "    train(model_dict, 'lgb')  #LGB prend bcp trop de temps à s'exécuter\n",
    "    print('lgb done')\n",
    "    train(model_dict, 'xgb')\n",
    "    print('xgb done')\n",
    "    train(model_dict, 'cbt')\n",
    "    print('cbt done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0a9bb3",
   "metadata": {
    "papermill": {
     "duration": 0.003096,
     "end_time": "2024-10-14T19:09:31.631571",
     "exception": false,
     "start_time": "2024-10-14T19:09:31.628475",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### There seems to be bug in official code, can only submit polars dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0007914a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-14T19:09:31.639804Z",
     "iopub.status.busy": "2024-10-14T19:09:31.639418Z",
     "iopub.status.idle": "2024-10-14T19:09:31.647723Z",
     "shell.execute_reply": "2024-10-14T19:09:31.646740Z"
    },
    "papermill": {
     "duration": 0.014825,
     "end_time": "2024-10-14T19:09:31.649775",
     "exception": false,
     "start_time": "2024-10-14T19:09:31.634950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lags_ : pl.DataFrame | None = None\n",
    "\n",
    "# Replace this function with your inference code.\n",
    "# You can return either a Pandas or Polars dataframe, though Polars is recommended.\n",
    "# Each batch of predictions (except the very first) must be returned within 10 minutes of the batch features being provided.\n",
    "def predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n",
    "    \"\"\"Make a prediction.\"\"\"\n",
    "    # All the responders from the previous day are passed in at time_id == 0. We save them in a global variable for access at every time_id.\n",
    "    # Use them as extra features, if you like.\n",
    "    global lags_\n",
    "    if lags is not None:\n",
    "        lags_ = lags\n",
    "\n",
    "    predictions = test.select(\n",
    "        'row_id',\n",
    "        pl.lit(0.0).alias('responder_6'),\n",
    "    )\n",
    "    \n",
    "    feat = test[feature_names].to_numpy()\n",
    "    \n",
    "    pred = [model.predict(feat) for model in models]\n",
    "    pred = np.mean(pred, axis=0)\n",
    "    \n",
    "    predictions = predictions.with_columns(pl.Series('responder_6', pred.ravel()))\n",
    "\n",
    "    # The predict function must return a DataFrame\n",
    "    assert isinstance(predictions, pl.DataFrame | pd.DataFrame)\n",
    "    # with columns 'row_id', 'responer_6'\n",
    "    assert list(predictions.columns) == ['row_id', 'responder_6']\n",
    "    # and as many rows as the test data.\n",
    "    assert len(predictions) == len(test)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baee2959",
   "metadata": {
    "papermill": {
     "duration": 0.00304,
     "end_time": "2024-10-14T19:09:31.656228",
     "exception": false,
     "start_time": "2024-10-14T19:09:31.653188",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "When your notebook is run on the hidden test set, inference_server.serve must be called within 15 minutes of the notebook starting or the gateway will throw an error. If you need more than 15 minutes to load your model you can do so during the very first `predict` call, which does not have the usual 10 minute response deadline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edfee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kaggle-environments\n",
    "# import kaggle_evaluation.jane_street_inference_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11aea3a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-14T19:09:31.665097Z",
     "iopub.status.busy": "2024-10-14T19:09:31.663986Z",
     "iopub.status.idle": "2024-10-14T19:09:32.109313Z",
     "shell.execute_reply": "2024-10-14T19:09:32.108185Z"
    },
    "papermill": {
     "duration": 0.452414,
     "end_time": "2024-10-14T19:09:32.111883",
     "exception": false,
     "start_time": "2024-10-14T19:09:31.659469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inference_server = kaggle_evaluation.jane_street_inference_server.JSInferenceServer(predict)\n",
    "\n",
    "# if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "#     inference_server.serve()\n",
    "# else:\n",
    "#     inference_server.run_local_gateway(\n",
    "#         (\n",
    "#             '/kaggle/input/jane-street-realtime-marketdata-forecasting/test.parquet',\n",
    "#             '/kaggle/input/jane-street-realtime-marketdata-forecasting/lags.parquet',\n",
    "#             # '/test.parquet',\n",
    "#             # '/lags.parquet',\n",
    "#         )\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72cd0a5",
   "metadata": {
    "papermill": {
     "duration": 0.003186,
     "end_time": "2024-10-14T19:09:32.118579",
     "exception": false,
     "start_time": "2024-10-14T19:09:32.115393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbc1834",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import required module for the inference server\n",
    "from kaggle_evaluation.jane_street_inference_server import JSInferenceServer\n",
    "\n",
    "# Setting up the inference server with the predict function\n",
    "inference_server = JSInferenceServer(predict)\n",
    "\n",
    "# Start the server to handle submissions\n",
    "if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    # For local testing (adjust paths as needed)\n",
    "    inference_server.run_local_gateway(\n",
    "        (\n",
    "            '/kaggle/input/jane-street-realtime-marketdata-forecasting/test.parquet',\n",
    "            '/kaggle/input/jane-street-realtime-marketdata-forecasting/lags.parquet',\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d3ba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import required module for the inference server\n",
    "from kaggle_evaluation.jane_street_inference_server import JSInferenceServer\n",
    "\n",
    "# Setting up the inference server with the predict function\n",
    "inference_server = JSInferenceServer(predict)\n",
    "\n",
    "# Start the server to handle submissions\n",
    "if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    # For local testing (adjust paths as needed)\n",
    "    inference_server.run_local_gateway(\n",
    "        (\n",
    "            '/kaggle/input/jane-street-realtime-marketdata-forecasting/test.parquet',\n",
    "            '/kaggle/input/jane-street-realtime-marketdata-forecasting/lags.parquet',\n",
    "        )\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9849268,
     "sourceId": 84493,
     "sourceType": "competition"
    },
    {
     "datasetId": 5875295,
     "sourceId": 9625192,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 33.857657,
   "end_time": "2024-10-14T19:09:32.944290",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-14T19:08:59.086633",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
